# LLM-benchmark
We benchmark real LLM [FLOPS](https://en.wikipedia.org/wiki/FLOPS) that training large lanugage models can achieve on various GPUs, including single GPU, multi-GPUs, and multi-machines. It helps you to estimate how many machine times you need to train your large-scale language models.

Training framework including supervised fine-tuning, reward modeding, and RLHF with PPO. 


## Reference 

[1] [transformers-benchmarks](https://github.com/mli/transformers-benchmarks)
[2] [alpaca](https://github.com/tatsu-lab/alpaca_farm) 
